import numpy as np
import scipy.sparse as sp
import torch
from torch import optim
from torch.nn import functional as F
from torch.nn.parameter import Parameter
from torch_geometric.utils import to_dense_adj, to_dense_batch, dense_to_sparse, degree, to_networkx, from_networkx
from tqdm import tqdm
# from deeprobust.graph.utils import normalize_adj_tensor
import copy
from torch.autograd import Variable
from torch_geometric.data import Data, Batch
import networkx as nx
import utils
from datasets.dataset_utils import label_regression2classification
from torch_geometric.loader import DataLoader
import random
from reconstruction.noise_optimization import NoiseGeneration

from torch.utils.tensorboard import SummaryWriter   

class kkt_reconstruction(torch.nn.Module):
    def __init__(self, cfg, dataset_infos, diffusion_model, classifier, datamodule, device):
        super(kkt_reconstruction, self).__init__()
        self.recon_config = cfg.reconstruct
        self.device = device
        self.diffusion_model = diffusion_model
        self.classifier = classifier.to(device)
        self.cfg = cfg
        self.datamodule = datamodule
        self.init_method = 'SDEdit' 
        self.reconstruct_method = 'diffusion'
        self.final_selection_method = 'KKT' 
        self.num_final_select = 100

        self.edge_attr_num = dataset_infos.output_dims['E']
        self.node_attr_num = dataset_infos.output_dims['X']

        self.num_classes = self.cfg.classifier['num_classes']
        self.num_generated_samples = self.recon_config['num_reconstruct_per_class'] * self.num_classes
        self.num_saved_samples = self.recon_config['sample_save_size']
        self.num_saved_chains = self.recon_config['chains_save_size']
        self.number_chain_steps = 1

        self.labmda_ = torch.rand(self.num_generated_samples).to(self.device)
        self.labmda_.requires_grad = True
        self.optimizer_l = torch.optim.AdamW([self.labmda_], lr=self.recon_config.extraction_lambda_lr, weight_decay=self.recon_config.extraction_lambda_weight_decay, amsgrad= True)

    def select_top_k_confident_samples(self, sampled_s_list, node_mask_list, molecule_list, k):
        self.classifier.eval()
        if k > len(molecule_list):
            raise ValueError("k cannot be greater than the length of the molecule list")
        scores = torch.FloatTensor([]).to(self.device)
        for i, sampled_s in enumerate(sampled_s_list):
            X,E,y = sampled_s.X, sampled_s.E, sampled_s.y
            print("X: ", X.shape, "y: ", y.shape)
            node_mask = node_mask_list[i]
            print(X.shape,E.shape)
            noisy_data = self.classifier.apply_noise(X, E, y, node_mask)
            extra_data = self.classifier.compute_extra_data(noisy_data)
            pred = self.classifier.forward(noisy_data, extra_data, node_mask)
            output = F.softmax(pred.y)
            scores = torch.concat([scores, output.max(dim=1).values], dim= 0)
            print(scores.detach().cpu().tolist())
        idx_score_sort = torch.argsort(scores, descending = True)
        print(idx_score_sort)
        idx_select = idx_score_sort[:k].cpu().numpy()
        
        select_samples = [molecule_list[i] for i in idx_select]
        return select_samples
    
    def confidence_selection1(self, dataset):
        # idx_candidate = [110240, 36738, 36740, 36741, 36742, 36743, 36744, 36745, 36746, 36747, 36748, 36749, 36750, 36751, 36752, 36753, 36754, 36755, 36756, 36757, 36758, 36759, 36760, 36761, 36762, 36739, 36737, 36764, 36736, 36713, 36714, 36715, 36716, 36717, 36718, 36719, 36720, 36721, 36722, 36723, 36724, 36725, 36726, 36727, 36728, 36729, 36730, 36731, 36732, 36733, 36734, 36735, 36763, 36765, 36711, 36792, 36794, 36795, 36796, 36797, 36798, 36799, 36800, 36801, 36802, 36803, 36804, 36805, 36806, 36807, 36808, 36809, 36810, 36811, 36812, 36813, 36814, 36815, 36816, 36793, 36791, 36766, 36790, 36767, 36768, 36769, 36770, 36771, 36772, 36773, 36774, 36775, 36776, 36777, 36778, 36779, 36780, 36781, 36782, 36783, 36784, 36785, 36786, 36787, 36788, 36789, 36712, 36710, 36603, 36630, 36632, 36633, 36634, 36635, 36636, 36637, 36638, 36639, 36640, 36641, 36642, 36643, 36644, 36645, 36646, 36647, 36648, 36649, 36650, 36651, 36652, 36653, 36654, 36631, 36629, 36656, 36628, 36605, 36606, 36607, 36608, 36609, 36610, 36611, 36612, 36613, 36614, 36615, 36616, 36617, 36618, 36619, 36620, 36621, 36622, 36623, 36624, 36625, 36626, 36627, 36655, 36657, 36709, 36684, 36686, 36687, 36688, 36689, 36690, 36691, 36692, 36693, 36694, 36695, 36696, 36697, 36698, 36699, 36700, 36701, 36702, 36703, 36704, 36705, 36706, 36707, 36708, 36685, 36683, 36658, 36682, 36659, 36660, 36661, 36662, 36663, 36664, 36665, 36666, 36667, 36668, 36669, 36670, 36671, 36672, 36673, 36674, 36675, 36676, 36677, 36678, 36679, 36680, 36681, 36817, 36818, 36819, 36953, 36955, 36956, 36957, 36958, 36959, 36960, 36961, 36962, 36963, 36964, 36965, 36966, 36967, 36968, 36969, 36970, 36971, 36972, 36973, 36974, 36975, 36976, 36977, 36954, 36952, 36979, 36951, 36928, 36929, 36930, 36931, 36932, 36933, 36934, 36935, 36936, 36937, 36938, 36939, 36940, 36941, 36942, 36943, 36944, 36945, 36946, 36947, 36948, 36949, 36950, 36978, 36980, 36820, 37007, 37009, 37010, 37011, 37012, 37013, 37014, 37015, 37016, 37017, 37018, 37019, 37020, 37021, 37022, 37023, 37024, 37025, 37026, 37027, 37028, 37029, 37030, 37031, 37008, 37006, 36981, 37005, 36982, 36983, 36984, 36985, 36986, 36987, 36988, 36989, 36990, 36991, 36992, 36993, 36994, 36995, 36996, 36997, 36998, 36999, 37000, 37001, 37002, 37003, 37004, 36927, 36926, 36925, 36846, 36848, 36849, 36850, 36851, 36852, 36853, 36854, 36855, 36856, 36857, 36858, 36859, 36860, 36861, 36862, 36863, 36864, 36865, 36866, 36867, 36868, 36869, 36870, 36847, 36845, 36924, 36844, 36821, 36822, 36823, 36824, 36825, 36826, 36827, 36828, 36829, 36830, 36831, 36832, 36833, 36834, 36835, 36836, 36837, 36838, 36839, 36840, 36841, 36842, 36843, 36871, 36872, 36873, 36874, 36901, 36902, 36903, 36904, 36905, 36906, 36907, 36908, 36909, 36910, 36911, 36912, 36913, 36914, 36915, 36916, 36917, 36918, 36919, 36920, 36921, 36922, 36923, 36900, 36899, 36898, 36885, 36875, 36876, 36877, 36878, 36879, 36880, 36881, 36882, 36883, 36884, 36886, 36897, 36887, 36888, 36889, 36890, 36891, 36892, 36893, 36894, 36895, 36896, 36604, 36602, 36172, 36307, 36309, 36310, 36311, 36312, 36313, 36314, 36315, 36316, 36317, 36318, 36319, 36320, 36321, 36322, 36323, 36324, 36325, 36326, 36327, 36328, 36329, 36330, 36331, 36308, 36306, 36333, 36305, 36282, 36283, 36284, 36285, 36286, 36287, 36288, 36289, 36290, 36291, 36292, 36293, 36294, 36295, 36296, 36297, 36298, 36299, 36300, 36301, 36302, 36303, 36304, 36332, 36334, 36280, 36361, 36363, 36364, 36365, 36366, 36367, 36368, 36369, 36370, 36371, 36372, 36373, 36374, 36375, 36376, 36377, 36378, 36379, 36380, 36381, 36382, 36383, 36384, 36385, 36362, 36360, 36335, 36359, 36336, 36337, 36338, 36339, 36340, 36341, 36342, 36343, 36344, 36345, 36346, 36347, 36348, 36349, 36350, 36351, 36352, 36353, 36354, 36355, 36356, 36357, 36358, 36281, 36279, 36601, 36199, 36201, 36202, 36203, 36204, 36205, 36206, 36207, 36208, 36209, 36210, 36211, 36212, 36213, 36214, 36215, 36216, 36217, 36218, 36219, 36220, 36221, 36222, 36223, 36200, 36198, 36225, 36197, 36174, 36175, 36176, 36177, 36178, 36179, 36180, 36181, 36182, 36183, 36184, 36185, 36186, 36187, 36188, 36189, 36190, 36191, 36192, 36193, 36194, 36195, 36196, 36224, 36226, 36278, 36253, 36255, 36256, 36257, 36258, 36259, 36260, 36261, 36262, 36263, 36264, 36265, 36266, 36267, 36268, 36269, 36270, 36271, 36272, 36273, 36274, 36275, 36276, 36277, 36254, 36252, 36227, 36251, 36228, 36229, 36230, 36231, 36232, 36233, 36234, 36235, 36236, 36237, 36238, 36239, 36240, 36241, 36242, 36243, 36244, 36245, 36246, 36247, 36248, 36249, 36250, 36386, 36387, 36388, 36522, 36524, 36525, 36526, 36527, 36528, 36529, 36530, 36531, 36532, 36533, 36534, 36535, 36536, 36537, 36538, 36539, 36540, 36541, 36542, 36543, 36544, 36545, 36546, 36523, 36521, 36548, 36520, 36497, 36498, 36499, 36500, 36501, 36502, 36503, 36504, 36505, 36506, 36507, 36508, 36509, 36510, 36511, 36512, 36513, 36514, 36515, 36516, 36517, 36518, 36519, 36547, 36549, 36389, 36576, 36578, 36579, 36580, 36581, 36582, 36583, 36584, 36585, 36586, 36587, 36588, 36589, 36590, 36591, 36592, 36593, 36594, 36595, 36596, 36597, 36598, 36599, 36600, 36577, 36575, 36550, 36574, 36551, 36552, 36553, 36554, 36555, 36556, 36557, 36558, 36559, 36560, 36561, 36562, 36563, 36564, 36565, 36566, 36567, 36568, 36569, 36570, 36571, 36572, 36573, 36496, 36495, 36494, 36415, 36417, 36418, 36419, 36420, 36421, 36422, 36423, 36424, 36425, 36426, 36427, 36428, 36429, 36430, 36431, 36432, 36433, 36434, 36435, 36436, 36437, 36438, 36439, 36416, 36414, 36493, 36413, 36390, 36391, 36392, 36393, 36394, 36395, 36396, 36397, 36398, 36399, 36400, 36401, 36402, 36403, 36404, 36405, 36406, 36407, 36408, 36409, 36410, 36411, 36412, 36440, 36441, 36442, 36443, 36470, 36471, 36472, 36473, 36474, 36475, 36476, 36477, 36478, 36479, 36480, 36481, 36482, 36483, 36484, 36485, 36486, 36487, 36488, 36489, 36490, 36491, 36492, 36469, 36468, 36467, 36454, 36444, 36445, 36446, 36447, 36448, 36449, 36450, 36451, 36452, 36453, 36455, 36466, 36456, 36457, 36458, 36459, 36460, 36461, 36462, 36463, 36464, 36465, 37032, 37033, 37034, 37599, 37601, 37602, 37603, 37604, 37605, 37606, 37607, 37608, 37609, 37610, 37611, 37612, 37613, 37614, 37615, 37616, 37617, 37618, 37619, 37620, 37621, 37622, 37623, 37600, 37598, 37625, 37597, 37574, 37575, 37576, 37577, 37578, 37579, 37580, 37581, 37582, 37583, 37584, 37585, 37586, 37587, 37588, 37589, 37590, 37591, 37592, 37593, 37594, 37595, 37596, 37624, 37626, 37572, 37653, 37655, 37656, 37657, 37658, 37659, 37660, 37661, 37662, 37663, 37664, 37665, 37666, 37667, 37668, 37669, 37670, 37671, 37672, 37673, 37674, 37675, 37676, 37677, 37654, 37652, 37627, 37651, 37628, 37629, 37630, 37631, 37632, 37633, 37634, 37635, 37636, 37637, 37638, 37639, 37640, 37641, 37642, 37643, 37644, 37645, 37646, 37647, 37648, 37649, 37650, 37573, 37571, 37035, 37491, 37493, 37494, 37495, 37496, 37497, 37498, 37499, 37500, 37501, 37502, 37503, 37504, 37505, 37506, 37507, 37508, 37509, 37510, 37511, 37512, 37513, 37514, 37515, 37492, 37490, 37517, 37489, 37466, 37467, 37468, 37469, 37470, 37471, 37472, 37473, 37474, 37475, 37476, 37477, 37478, 37479, 37480, 37481, 37482, 37483, 37484, 37485, 37486, 37487, 37488, 37516, 37518, 37570, 37545, 37547, 37548, 37549, 37550, 37551, 37552, 37553, 37554, 37555, 37556, 37557, 37558, 37559, 37560, 37561, 37562, 37563, 37564, 37565, 37566, 37567, 37568, 37569, 37546, 37544, 37519, 37543, 37520, 37521, 37522, 37523, 37524, 37525, 37526, 37527, 37528, 37529, 37530, 37531, 37532, 37533, 37534, 37535, 37536, 37537, 37538, 37539, 37540, 37541, 37542, 37678, 37679, 37680, 37814, 37816, 37817, 37818, 37819, 37820, 37821, 37822, 37823, 37824, 37825, 37826, 37827, 37828, 37829, 37830, 37831, 37832, 37833, 37834, 37835, 37836, 37837, 37838, 37815, 37813, 37840, 37812, 37789, 37790, 37791, 37792, 37793, 37794, 37795, 37796, 37797, 37798, 37799, 37800, 37801, 37802, 37803, 37804, 37805, 37806, 37807, 37808, 37809, 37810, 37811, 37839, 37841, 37681, 37868, 37870, 37871, 37872, 37873, 37874, 37875, 37876, 37877, 37878, 37879, 37880, 37881, 37882, 37883, 37884, 37885, 37886, 37887, 37888, 37889, 37890, 37891, 37892, 37869, 37867, 37842, 37866, 37843, 37844, 37845, 37846, 37847, 37848, 37849, 37850]
        idx_candidate = [ 73485,  86415,  89385,  76834,  73746,  73243,  73315,  76513,  76810,  75572, 73097, 113389,  77298,  87332,  76664,  83167,  84776,  73348,  73302,  73361, 112750,  89655, 113507,  76684,  76397,  75719,  73977,  76306,  76314,  76906, 76680, 115880,  75080,  80451,  89594, 113973,  76754,  75905,     29,  82616, 97740, 115626,  86575,      5,  76201,  76715,    111,    528,     68,  76183, 533,  97736,  75692,  75670,  97821, 113421,  75148,  74587,   2613,      1, 2659,  25041,  77311,  76070,  76051,   2424,  73108,  79554,  25420,  98995, 1892,   8396,   2384,  24861,  26225,  56764,  24747,  25641,  67055,  12891, 98261,  81697,   2367,  26271,   2266,  75653,  14761,  66241,  65842,  47447, 38670,  38978,  24333,   8627,  75760, 100674,  46746,  38626,  47127,   1603, 47795,   1767,  38706,    477,   9559,  73174,  11150, 103076,  42663,   1233, 38745,  73920,   4967,   5376,  25687,   7755,   1787,  39155,  41749,  24001, 71609,   7167,  85754,  13929,  26314,  12903,  48643,  86867,  75804,  48113, 519,  38822,  55771,  64621,   1445,  62941,  25794,  44892,  62798,  74160, 38667,  25856,   5758,  59462,  11019,  76985,   2529,  14771,  63278,  26205, 2251, 106142, 102745,  84825,   4325,  50238,  42353,  63598,    490, 112427, 2629,  46432,  56145,  85782,    227,  97732,   6822,   1778,  24671,  26654, 63,   2652,  13912,  11316,  24023, 100071,  26024,  10111,  12508,   9901, 25319,  75454,   8560,    500,     53,    286,  50695,  86523,  99553,  26619, 4761,  24223,  59208,  41151,  59320,  65659,  68783,  13701,   8798,  98173, 11949,  46211,  24078,   1681,  50452, 113833,  10105,  63038,  70636,  60953, 24653,   2552,  56074,  38802,   9363,   2355,  87109,  24254,   4829, 105724, 6180,  38564, 100384,  97753,  76327,  71542,  39090,    496,  24444,  82848, 103348,  74931,     47,  76997,  43231, 100527,   2653,  49841, 107088,  26094, 39184,  24633,   2478,  87279,   1739,  13114,    211,  75154,  39170,  24592, 24554,  71480, 102863,  70229, 105716,  61309,  53867,     79,    109,   1181, 47370,  26410,  42699,  67665,  71753,  26354,  71520,  74828,   2570,    301, 13791,    407,  99223,   2381,     18,  58849,  43575,  39037,  66811,  58018, 13766,    460, 105610,  75427,    406,  62823,  69032,  86661, 113083,  24094, 46822,   1063,  47331,  75924,  87346,   2664,   2580,  76076,  24543, 115387, 79321,    787,  48494,    848,  46343,  55904,  52407,  57732,  27748,  29374, 1653,    998,   6240, 105566,  99348,   1937,  12760,  38738,  90202,   2555, 12737,    480,   2406,  90147,  49006, 102962,  14751,    384,   5056,   1760, 51488,  24369,  13834,  72440,  39640,  40807,  62150,  40386, 112574,  40655, 44015,  68121,  39739,  40769,  74556,  40899,  89411,  40054,  43673,  90297, 39818,  63560,  39796,  40394,  86285, 105955,  40315,  65201, 115547,  40931, 40278,  40004, 105871,  39618,  86674,  40911,  10042,  73473,  44836,  75168, 90096,  42417,  40434,  92747,  70540,  85779,  72721,  91330,  82853,  68829, 42085, 112851, 115478,  92788,  85344, 103977,  85112,  92653,  92495, 112715, 39646,   4372,   4871,  46981, 116468,  63618,  40819,  40087,  83525,  70042, 85652,   5299,  91277,  62998, 106345,  71472,  44764, 115273, 116745,  92771, 56125, 116203,  70538,  69306,  61428, 112377,  75062,  87357,  43508,  93539, 28616,  70713,  90280, 112204,  71940,  28561,  92531, 106693,  77281,  72734, 116710,  92532,  89227,  44176,  30460,  40700,  85669,  71171,  29191,  92673, 46899,  91325,  71767,   4342,  86722,  76425,  70040,  68213,  92082, 106302, 34766,  71397,  44153,  68254, 116790,  85662,  88535,  91831, 103790, 116302, 51651,  27802,  29231,  51535, 115234, 106303,  85683, 116562,   7467,  91561, 93395, 112142,  71713,  74665, 113100,  73228,  72883,  43281,  90361,  70765, 99619,  73048,  51652,  82243,  71390,  63451,  92664,  91347,  85657,  98771, 92535,  89143,  40550,  89262, 112761,  72455,  92498,  71048,  92732,  71173, 91281,  13050,  68541, 112481, 110259, 112375,  71225,  92822, 105743, 112371, 30384,  80033,  68246,  91308,  91388,  74397,  85343,  83007,  71159,  70523, 110281,   9098,  43915,  85897,  92801,  39439, 112363,  77063,  39413,  51536, 84420, 112820, 106335,  68819, 112321,  15179, 105909,  15204,  34907,  92944, 92034,  81282,  88980,   9103,  69013,  64541, 116628,  40745,  92917,  59879, 63773,  51271,  55004, 114164,  92508,  49988,  91374,  72044,  55243,  63261, 40678,  30461,   8370,  39121,  40178,  40606,  72571, 116653,  72292,  81916, 42675,  85882,  55708,  40345, 116470,  34448,  40666,  71039,  70015,  91331, 71188, 115374, 107111,  91573, 116743, 106113,  68220,  41709,  90759,  72765, 39697,  62993,  71051,  30399, 116693,  42208,  92792,  71374, 106834,  72124, 116517,  92969,  69980,  71184, 106961,  98622,  92475,  67598,  83581,  39730, 116615,  60934,  73669,  71404, 116303,  72166,  92728,  71000,  71163, 107917, 16626,  80663,  38421,  55930,  90768,  90532,  78793,  16254,  16411,  30393, 105570, 105616,  89309,  45323,  80154,  39659,  16211,  92525,  92378,  40094, 112357,  16715,  15181,  71005,  74611,  27478,  61692,  16255,  92308,  27479, 76426,  93526,  27484,  40591,  62241,  92748,  71246,  59055,  91311,  40241, 4601, 115394,  47012,  99963,  40368,  53584,  54384,  97800,  38434,  38962, 39134,  74829,  99750,  38614,  39281,  32548,  38407,  79820, 104571,  38505, 10681, 100040,   9618,  12904,   5985,  13120,  25050,   9925,   4810,   7138, 25055,  10113,  98986,  26450,  38402, 100145,  62308, 100041,  13436,  98777, 25230,  11154,   5349,  25656,  25277,  11287,   9750,  38567, 109470,  25037, 25143,  25579, 102935, 115745,  14211,  26091,  25087,  71496,   7578,  52038, 103005,  79745, 102943, 102963,  71787,  49147,   9501,  25244,  54540,  25894, 25422, 107769,  25093,  86190,  25070,   4214,  25339, 107706,  39044,  25218, 25387,   5380,  25966,  25468,  85798, 106488,  49059,  61314,   9674,  51349, 25501,  59981,  45738,  25807,  25482, 103103,   4408,  59663,  25131, 108242, 24905,  25821, 109707,  26216,   7070,  57180,  26227,  26241,  25264, 103149, 103054,  60994,  25332,  81287,  26157,  50112, 107466, 110923,  26097,  55694, 52233,  67365,  81568,  53557,  41485,  65282,  25253,  38458,   5160,  66189, 65745,  38919,  25169, 103167,  25591,  25847,  25539,  38768,  49965,  13214, 46477, 103058,   5236,  53873,  26168,  45825,  57086,   4826,   8308,  50716, 49694,  26267,  26116,  57563,  25876,  66245,  24407,  64289,  50243,  26338, 6700, 108522,  24350,  24097, 107093,  67134,  26315,  26358, 107348,  26345, 78474,  49800,  98948,  25611, 107429,  60190,  98630,  59327,  62805, 106339, 111887,  53455,  59092,  26371,   8112,  26297,  26331,  41329,  99827,  41314, 79451,  38803,  33980,  41395,  47160,  55749,  11531,  25313,  55722, 110632, 6947,  43235,  79660,  59804,  46546,  51492,  26556,  45760, 109212,  62814, 50652, 109762,  99576, 110213,  46943,  26066,  26176,  65622,  98808,  47200, 50373,  24282,  26489,  47765,  67058,  57016,  26529,  52270, 103048,  14581, 26140,  59159, 111456,  49887,  49756,  43468,  13170, 111337,  66287, 106126, 107212,  54196, 103215,  50087,  59459,  13226,  24656,  24666,  55121,  72980, 41671,  64964,  24685,  24711,   4946,  10836,  12418,  80331,  24233,  24496, 106247,  13395, 108438,  26104,  24428,  74880,  50707,   4517, 115591,  41254, 76351,  41367,  24258, 110578,  46750, 106505, 102778,  69549,  86226,  52655, 4177,  10721,  48689, 107543,  39191,  68788,  88897,  41536, 103201, 102814, 26500, 102896,  49217,  10847,  71517,   6434, 100059,  24642,  62852,  24696, 41575,  26009,  54745, 113136,  42092,  56438, 115901, 116148,  84882, 107367, 70639,  75519,  54595,   9828, 110188,  79624,  44906,  24739,  58982,  51982, 56772,  44866, 106680,  70359,  10157,  76915,  73005,  80774,  57657, 112708, 110713,  41946,  48960,  80550,  39239,  68929,  80671,  38903,  58023]
        idx_candidate = np.array(idx_candidate)
        idx_candidate = idx_candidate[:999]
        can_data_list = []
        for idx in idx_candidate:
            can_data_list.append(dataset[idx])
        
        from torch.utils.data import Subset

        samples_left_to_generate = self.num_generated_samples
        bs = 2 * self.recon_config.sample_batch_size

        can_dataloader = DataLoader(Subset(dataset, idx_candidate), batch_size = bs)

        to_generate = min(samples_left_to_generate, bs)

        X_list = []
        E_list = []
        y_list = []
        print(can_dataloader)
        for data in can_dataloader:
            data = data.to(self.device)
            y = data.y
            y = torch.unsqueeze(y, dim = 1)
            dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)
            dense_data = dense_data.mask(node_mask)
            X, E = dense_data.X, dense_data.E
            X_list.append(X)
            E_list.append(E)
            y_list.append(y)
        return X_list, E_list, y_list
            
    def alternative_selection(self, dataset):
        # idx_candidate = [ 73485,  86415,  89385,  76834,  73746,  73243,  73315,  76513,  76810,  75572, 73097, 113389,  77298,  87332,  76664,  83167,  84776,  73348,  73302,  73361, 112750,  89655, 113507,  76684,  76397,  75719,  73977,  76306,  76314,  76906, 76680, 115880,  75080,  80451,  89594, 113973,  76754,  75905,     29,  82616, 97740, 115626,  86575,      5,  76201,  76715,    111,    528,     68,  76183, 533,  97736,  75692,  75670,  97821, 113421,  75148,  74587,   2613,      1, 2659,  25041,  77311,  76070,  76051,   2424,  73108,  79554,  25420,  98995, 1892,   8396,   2384,  24861,  26225,  56764,  24747,  25641,  67055,  12891, 98261,  81697,   2367,  26271,   2266,  75653,  14761,  66241,  65842,  47447, 38670,  38978,  24333,   8627,  75760, 100674,  46746,  38626,  47127,   1603, 47795,   1767,  38706,    477,   9559,  73174,  11150, 103076,  42663,   1233, 38745,  73920,   4967,   5376,  25687,   7755,   1787,  39155,  41749,  24001, 71609,   7167,  85754,  13929,  26314,  12903,  48643,  86867,  75804,  48113, 519,  38822,  55771,  64621,   1445,  62941,  25794,  44892,  62798,  74160, 38667,  25856,   5758,  59462,  11019,  76985,   2529,  14771,  63278,  26205, 2251, 106142, 102745,  84825,   4325,  50238,  42353,  63598,    490, 112427, 2629,  46432,  56145,  85782,    227,  97732,   6822,   1778,  24671,  26654, 63,   2652,  13912,  11316,  24023, 100071,  26024,  10111,  12508,   9901, 25319,  75454,   8560,    500,     53,    286,  50695,  86523,  99553,  26619, 4761,  24223,  59208,  41151,  59320,  65659,  68783,  13701,   8798,  98173, 11949,  46211,  24078,   1681,  50452, 113833,  10105,  63038,  70636,  60953, 24653,   2552,  56074,  38802,   9363,   2355,  87109,  24254,   4829, 105724, 6180,  38564, 100384,  97753,  76327,  71542,  39090,    496,  24444,  82848, 103348,  74931,     47,  76997,  43231, 100527,   2653,  49841, 107088,  26094, 39184,  24633,   2478,  87279,   1739,  13114,    211,  75154,  39170,  24592, 24554,  71480, 102863,  70229, 105716,  61309,  53867,     79,    109,   1181, 47370,  26410,  42699,  67665,  71753,  26354,  71520,  74828,   2570,    301, 13791,    407,  99223,   2381,     18,  58849,  43575,  39037,  66811,  58018, 13766,    460, 105610,  75427,    406,  62823,  69032,  86661, 113083,  24094, 46822,   1063,  47331,  75924,  87346,   2664,   2580,  76076,  24543, 115387, 79321,    787,  48494,    848,  46343,  55904,  52407,  57732,  27748,  29374, 1653,    998,   6240, 105566,  99348,   1937,  12760,  38738,  90202,   2555, 12737,    480,   2406,  90147,  49006, 102962,  14751,    384,   5056,   1760, 51488,  24369,  13834,  72440,  39640,  40807,  62150,  40386, 112574,  40655, 44015,  68121,  39739,  40769,  74556,  40899,  89411,  40054,  43673,  90297, 39818,  63560,  39796,  40394,  86285, 105955,  40315,  65201, 115547,  40931, 40278,  40004, 105871,  39618,  86674,  40911,  10042,  73473,  44836,  75168, 90096,  42417,  40434,  92747,  70540,  85779,  72721,  91330,  82853,  68829, 42085, 112851, 115478,  92788,  85344, 103977,  85112,  92653,  92495, 112715, 39646,   4372,   4871,  46981, 116468,  63618,  40819,  40087,  83525,  70042, 85652,   5299,  91277,  62998, 106345,  71472,  44764, 115273, 116745,  92771, 56125, 116203,  70538,  69306,  61428, 112377,  75062,  87357,  43508,  93539, 28616,  70713,  90280, 112204,  71940,  28561,  92531, 106693,  77281,  72734, 116710,  92532,  89227,  44176,  30460,  40700,  85669,  71171,  29191,  92673, 46899,  91325,  71767,   4342,  86722,  76425,  70040,  68213,  92082, 106302, 34766,  71397,  44153,  68254, 116790,  85662,  88535,  91831, 103790, 116302, 51651,  27802,  29231,  51535, 115234, 106303,  85683, 116562,   7467,  91561, 93395, 112142,  71713,  74665, 113100,  73228,  72883,  43281,  90361,  70765, 99619,  73048,  51652,  82243,  71390,  63451,  92664,  91347,  85657,  98771, 92535,  89143,  40550,  89262, 112761,  72455,  92498,  71048,  92732,  71173, 91281,  13050,  68541, 112481, 110259, 112375,  71225,  92822, 105743, 112371, 30384,  80033,  68246,  91308,  91388,  74397,  85343,  83007,  71159,  70523, 110281,   9098,  43915,  85897,  92801,  39439, 112363,  77063,  39413,  51536, 84420, 112820, 106335,  68819, 112321,  15179, 105909,  15204,  34907,  92944, 92034,  81282,  88980,   9103,  69013,  64541, 116628,  40745,  92917,  59879, 63773,  51271,  55004, 114164,  92508,  49988,  91374,  72044,  55243,  63261, 40678,  30461,   8370,  39121,  40178,  40606,  72571, 116653,  72292,  81916, 42675,  85882,  55708,  40345, 116470,  34448,  40666,  71039,  70015,  91331, 71188, 115374, 107111,  91573, 116743, 106113,  68220,  41709,  90759,  72765, 39697,  62993,  71051,  30399, 116693,  42208,  92792,  71374, 106834,  72124, 116517,  92969,  69980,  71184, 106961,  98622,  92475,  67598,  83581,  39730, 116615,  60934,  73669,  71404, 116303,  72166,  92728,  71000,  71163, 107917, 16626,  80663,  38421,  55930,  90768,  90532,  78793,  16254,  16411,  30393, 105570, 105616,  89309,  45323,  80154,  39659,  16211,  92525,  92378,  40094, 112357,  16715,  15181,  71005,  74611,  27478,  61692,  16255,  92308,  27479, 76426,  93526,  27484,  40591,  62241,  92748,  71246,  59055,  91311,  40241, 4601, 115394,  47012,  99963,  40368,  53584,  54384,  97800,  38434,  38962, 39134,  74829,  99750,  38614,  39281,  32548,  38407,  79820, 104571,  38505, 10681, 100040,   9618,  12904,   5985,  13120,  25050,   9925,   4810,   7138, 25055,  10113,  98986,  26450,  38402, 100145,  62308, 100041,  13436,  98777, 25230,  11154,   5349,  25656,  25277,  11287,   9750,  38567, 109470,  25037, 25143,  25579, 102935, 115745,  14211,  26091,  25087,  71496,   7578,  52038, 103005,  79745, 102943, 102963,  71787,  49147,   9501,  25244,  54540,  25894, 25422, 107769,  25093,  86190,  25070,   4214,  25339, 107706,  39044,  25218, 25387,   5380,  25966,  25468,  85798, 106488,  49059,  61314,   9674,  51349, 25501,  59981,  45738,  25807,  25482, 103103,   4408,  59663,  25131, 108242, 24905,  25821, 109707,  26216,   7070,  57180,  26227,  26241,  25264, 103149, 103054,  60994,  25332,  81287,  26157,  50112, 107466, 110923,  26097,  55694, 52233,  67365,  81568,  53557,  41485,  65282,  25253,  38458,   5160,  66189, 65745,  38919,  25169, 103167,  25591,  25847,  25539,  38768,  49965,  13214, 46477, 103058,   5236,  53873,  26168,  45825,  57086,   4826,   8308,  50716, 49694,  26267,  26116,  57563,  25876,  66245,  24407,  64289,  50243,  26338, 6700, 108522,  24350,  24097, 107093,  67134,  26315,  26358, 107348,  26345, 78474,  49800,  98948,  25611, 107429,  60190,  98630,  59327,  62805, 106339, 111887,  53455,  59092,  26371,   8112,  26297,  26331,  41329,  99827,  41314, 79451,  38803,  33980,  41395,  47160,  55749,  11531,  25313,  55722, 110632, 6947,  43235,  79660,  59804,  46546,  51492,  26556,  45760, 109212,  62814, 50652, 109762,  99576, 110213,  46943,  26066,  26176,  65622,  98808,  47200, 50373,  24282,  26489,  47765,  67058,  57016,  26529,  52270, 103048,  14581, 26140,  59159, 111456,  49887,  49756,  43468,  13170, 111337,  66287, 106126, 107212,  54196, 103215,  50087,  59459,  13226,  24656,  24666,  55121,  72980, 41671,  64964,  24685,  24711,   4946,  10836,  12418,  80331,  24233,  24496, 106247,  13395, 108438,  26104,  24428,  74880,  50707,   4517, 115591,  41254, 76351,  41367,  24258, 110578,  46750, 106505, 102778,  69549,  86226,  52655, 4177,  10721,  48689, 107543,  39191,  68788,  88897,  41536, 103201, 102814, 26500, 102896,  49217,  10847,  71517,   6434, 100059,  24642,  62852,  24696, 41575,  26009,  54745, 113136,  42092,  56438, 115901, 116148,  84882, 107367, 70639,  75519,  54595,   9828, 110188,  79624,  44906,  24739,  58982,  51982, 56772,  44866, 106680,  70359,  10157,  76915,  73005,  80774,  57657, 112708, 110713,  41946,  48960,  80550,  39239,  68929,  80671,  38903,  58023]
        # idx_candidate = np.array(idx_candidate)
        # can_data_list = []
        # for idx in idx_candidate:
        #     can_data_list.append(dataset[idx])
        
        # from torch.utils.data import Subset

        # samples_left_to_generate = self.num_generated_samples
        # bs = 2 * self.recon_config.sample_batch_size
        # can_dataset = Subset(dataset, idx_candidate)

        can_dataset = torch.load('./saved_graphs/opt_input_noise.pt')
        # noise_optimizer = NoiseGeneration(can_dataset, self.classifier, self.device)
        # can_dataset = noise_optimizer.reconstruct()
        can_dataloader = DataLoader(can_dataset, batch_size = bs)

        to_generate = min(samples_left_to_generate, bs)

        X_list = []
        E_list = []
        y_list = []
        for data in can_dataloader:
            data = data.to(self.device)
            y = data.y
            y = torch.unsqueeze(y, dim = 1)

            
            
            dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)
            dense_data = dense_data.mask(node_mask)
            X, E = dense_data.X, dense_data.E
            X_noise, E_noise, y = self.diffusion_model.apply_SDEdit_noise(X, E, y, node_mask, t = 100)
            X_list.append(X_noise)
            E_list.append(E_noise)
            y_list.append(y)
        return X_list, E_list, y_list

    def confidence_selection(self,dataset):
        with torch.no_grad():
            label_wise_indices = {}
            unique_classes = np.array(list(range(self.num_classes)))

            '''convert the continuous values to the categorial values'''
            targets = torch.FloatTensor([])
            for i in range(len(dataset)):
                data = dataset[i]
                target = data.y
                targets = torch.cat([targets,target], dim= 0)
            ys = targets
            for c in unique_classes:
                y_idx_list = []
                    
                for i in range(len(dataset)):
                    data = dataset[i]
                    y = ys[i]
                    if(y == c):
                        y_idx_list.append(i)
                y_idx_list = np.array(y_idx_list)
                conf_scores = []
                for i in y_idx_list:
                    data = dataset[i].to(self.device)
                    if(data.x.shape[0] == 1):
                        continue
                    target = ys[i]
                    data.y = torch.zeros(data.y.shape[0], 0).type_as(data.y)
                    dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)
                    dense_data = dense_data.mask(node_mask)
                    X, E = dense_data.X, dense_data.E
                    noisy_data = self.classifier.apply_noise(X, E, data.y, node_mask)
                    extra_data = self.classifier.compute_extra_data(noisy_data)
                    pred = self.classifier.forward(noisy_data, extra_data, node_mask)
                    output = F.softmax(pred.y)
                    conf_score = output[0][target].item()
                    conf_scores.append(conf_score)
                conf_scores = np.array(conf_scores)
                # sort conf_scores in descending order and get corresponding indices
                sorted_y_idx = y_idx_list[np.argsort(conf_scores)[::-1]]
                label_wise_indices[c] = sorted_y_idx.tolist() #sorted_y_idx[:n]
            
            print(label_wise_indices)
            idx_candidate = np.array([])
            num_candidate = self.recon_config['num_reconstruct_per_class'] # * self.num_classes
            for key in label_wise_indices.keys():
                idx_candidate = np.hstack((idx_candidate, np.array(label_wise_indices[key])[:num_candidate]))
                idx_candidate = idx_candidate.astype(int)
            print('idx candidate: {}'.format(idx_candidate))
            
            can_data_list = []
            for idx in idx_candidate:
                can_data_list.append(dataset[idx])
            
            from torch.utils.data import Subset

            samples_left_to_generate = self.num_generated_samples
            bs = 2 * self.recon_config.sample_batch_size
            can_dataloader = DataLoader(Subset(dataset, idx_candidate), batch_size = bs)

            to_generate = min(samples_left_to_generate, bs)

            X_list = []
            E_list = []
            y_list = []

            print(can_dataloader)
            for data in can_dataloader:
                data = data.to(self.device)
                y = data.y
                dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)
                dense_data = dense_data.mask(node_mask)
                X, E = dense_data.X, dense_data.E
                X_list.append(X)
                E_list.append(E)
                y_list.append(y)
            return X_list, E_list, y_list
        

    def select_top_k_KKT_samples(self, sampled_s_list, node_mask_list, molecule_list, k):
        sampled_ss = torch.FloatTensor([]).to(self.device)
        node_masks = torch.FloatTensor([]).to(self.device)

        for epoch in range(self.recon_config.num_recon_epoch):
            kkt_loss, lambda_loss = self.get_selection_loss(sampled_s_list, node_mask_list)
            if((epoch+1) % 1 == 0):
                print("Epoch: {} KKT loss: {:.4f} Lambda loss: {:.4f}".format(epoch, kkt_loss, lambda_loss))
            reconstruct_loss = kkt_loss + lambda_loss

            
            self.optimizer_l.zero_grad()
            reconstruct_loss.backward()
            self.optimizer_l.step()
        idx_score_sort = torch.argsort(self.labmda_, descending = True)
        print(idx_score_sort)
        idx_select = idx_score_sort[:k].cpu().numpy()
        select_samples = [molecule_list[i] for i in idx_select]
        return select_samples
    
    def reconstruct(self, ):
        torch.autograd.set_detect_anomaly(True)
        self.diffusion_model.eval()
        # self.diffusion_model.train()
        self.classifier.eval()
        num_recon_epoch = self.recon_config.num_recon_epoch
        num_classes = self.cfg.classifier['num_classes']

        samples_left_to_generate = self.num_generated_samples
        if(self.init_method == 'noisy'):
            X_list = []
            E_list = []
            y_list = []
            z_t_list = []
            samples_left_to_generate = self.num_generated_samples
            while samples_left_to_generate > 0:
                bs = 2 * self.recon_config.sample_batch_size
                to_generate = min(samples_left_to_generate, bs)
                z_t = self.diffusion_model.initialize_reconstruct_batch(batch_size=to_generate, batch_y = None)
                X_list.append((z_t.X.clone().detach().requires_grad_(True)))
                E_list.append(z_t.E.clone().detach().requires_grad_(True))
                y_list.append(z_t.y)
                z_t_list.append(z_t)
                samples_left_to_generate -= to_generate
        elif(self.init_method == 'SDEdit'):
            # X_list, E_list, y_list = self.alternative_selection(self.datamodule.test_dataset+self.datamodule.val_dataset)
            X_list, E_list, y_list = self.confidence_selection1(self.datamodule.test_dataset+self.datamodule.val_dataset)
        if(self.reconstruct_method == 'diffusion'):
            final_samples = []
            sampled_s_list = []
            node_mask_list = []
            '''Sampling'''
            samples_left_to_generate = self.num_generated_samples
            for batch_id in range(len(X_list)):
                y = torch.empty((X_list[batch_id].shape[0],0)).to(self.device)
                z_T = utils.PlaceHolder(X_list[batch_id], E_list[batch_id], y)
                print(X_list[batch_id].shape, E_list[batch_id].shape, y.shape)
                ident = 0
                bs = 2 * self.recon_config.sample_batch_size
                to_generate = min(samples_left_to_generate, bs)
                to_save = 10
                chains_save = 0
                samples_left_to_generate -= to_generate
                print(samples_left_to_generate)

                molecule_list, sampled_s, node_mask = self.diffusion_model.sample_batch(batch_id=ident, batch_size=to_generate, num_nodes=None,
                                                            save_final=to_save,
                                                            keep_chain=chains_save,
                                                            number_chain_steps=self.number_chain_steps, z_T = z_T)
                sampled_s_list.append(sampled_s)
                final_samples.extend(molecule_list)
                node_mask_list.append(node_mask)
                ident += to_generate

            print(sampled_s_list, node_mask_list)    
            
            if(self.final_selection_method == 'KKT'):
                final_samples = self.select_top_k_KKT_samples(sampled_s_list, node_mask_list, final_samples, k = self.num_final_select)
            else:
                raise NotImplementedError
            import os
            current_path = os.getcwd()
            result_path = os.path.join(current_path,
                                       f'graphs/{self.diffusion_model.name}/')
            self.diffusion_model.visualization_tools.visualize(result_path, final_samples, 100)
            return final_samples
    
    def get_selection_loss(self, generate_samples, node_mask_list):
        '''
        generate_samples: 
            type: list
            description: a list of the batch of recon_Gs
        
        recon_Gs:
            type: untils.PlaceHolder
            recon_G.X: [bn, nx, dx]
            recon_G.E: [bn, ne, ne, de]
        
        node_mask_list: 
            type: list
            description: a list of node_mask
        
        node_mask:
            type: torch.tensor
            description: 
        '''
        if(self.cfg.classifier.num_classes > 2):

            values = torch.zeros(self.num_generated_samples).to(self.device)
            recons_labels = -1 * torch.ones(self.num_generated_samples).to(self.device) 
            pred_y = torch.zeros(0, self.num_classes).to(self.device)

            for i, recon_Gs in enumerate(generate_samples):
                Xs = recon_Gs.X
                Es = recon_Gs.E
                ys = recon_Gs.y


                node_mask = node_mask_list[i]
                noisy_data = self.classifier.apply_noise(Xs, Es, ys, node_mask)
                extra_data = self.classifier.compute_extra_data(noisy_data)  # The regressor model should not be used with extra features
                pred = self.classifier.forward(noisy_data, extra_data, node_mask)
                pred_y = torch.concat([pred_y, pred.y], dim = 0)

            output = F.softmax(pred_y, dim=1)
            top2_values, top2_indices = torch.topk(output, 2, dim=1)
            values = top2_values[:,0] - top2_values[:,1]
            
            top_values, top_indices = torch.topk(self.labmda_.detach().clone(), self.num_final_select)
            mask = torch.zeros_like(self.labmda_).to(self.device)
            mask[top_indices] = 1
            filter_selection_mask = self.labmda_.detach().clone() * mask
 
            values = values * filter_selection_mask

            kkt_loss, lambda_loss = self.cal_extraction_loss(values, self.labmda_, recons_labels, self.classifier, loss_type = 'kkt')  
            return kkt_loss, lambda_loss

    def get_reconstruction_loss(self, generate_samples, node_mask_list):
        '''
        generate_samples: 
            type: list
            description: a list of the batch of recon_Gs
        
        recon_Gs:
            type: untils.PlaceHolder
            recon_G.X: [bn, nx, dx]
            recon_G.E: [bn, ne, ne, de]
        
        node_mask_list: 
            type: list
            description: a list of node_mask
        
        node_mask:
            type: torch.tensor
            description: 
        '''
        if(self.cfg.classifier.num_classes > 2):

            values = torch.zeros(self.num_generated_samples).to(self.device)
            recons_labels = -1 * torch.ones(self.num_generated_samples).to(self.device) 
            pred_y = torch.zeros(0, self.num_classes).to(self.device)
            for i, recon_Gs in enumerate(generate_samples):
                Xs = recon_Gs.X
                Es = recon_Gs.E
                ys = recon_Gs.y
                node_mask = node_mask_list[i]
               

                noisy_data = self.classifier.apply_noise(Xs, Es, ys, node_mask)
                extra_data = self.classifier.compute_extra_data(noisy_data)  # The regressor model should not be used with extra features
                pred = self.classifier.forward(noisy_data, extra_data, node_mask)
                pred_y = torch.concat([pred_y, pred.y], dim = 0)
            output = F.softmax(pred_y, dim=1)
            top2_values, top2_indices = torch.topk(output, 2, dim=1)
            values = top2_values[:,0] - top2_values[:,1]
            kkt_loss, lambda_loss = self.cal_extraction_loss(values, self.labmda_, recons_labels, self.classifier, loss_type = 'kkt')  
            return kkt_loss, lambda_loss
        
    def get_kkt_loss(self, values, lambda_, labels_recons, model, type='binary'):
        if(type == 'binary'):
            outputs = values * lambda_ * labels_recons
        elif(type == 'multi'):
            outputs = values * lambda_
        
        grad = torch.autograd.grad(outputs = outputs,
                                    inputs = model.parameters(),
                                    grad_outputs = torch.ones_like(outputs, requires_grad = False, device = self.device).div(self.num_generated_samples),
                                    retain_graph = True,
                                    create_graph = True,
                                    allow_unused=True)
        names = []
        loss = 0

        for i, (p, grad) in enumerate(zip(model.parameters(), grad)):
            if(grad is not None):
                assert p.shape == grad.shape
                l = (p.detach().data - grad).pow(2).sum()
                loss += l
        return loss

    def get_lambda_loss(self, lambda_):
        lambda_loss = (-lambda_ + self.recon_config.extraction_min_lambda).relu().pow(2).sum()

        return lambda_loss
    def cal_extraction_loss(self, values, lambda_, labels_recons, model, loss_type = 'kkt'):
        if(loss_type == 'kkt'):
            if(self.num_classes == 2):
                kkt_loss = self.get_kkt_loss(values, lambda_, labels_recons, model, type='binary')
            else:
                kkt_loss = self.get_kkt_loss(values, lambda_, labels_recons, model, type='multi')
            lambda_loss = self.get_lambda_loss(lambda_)

            # loss = kkt_loss + lambda_loss
            # return loss
            return kkt_loss, lambda_loss
        elif(loss_type == 'naive'):
            pass 

class GradWhere(torch.autograd.Function):
    """
    We can implement our own custom autograd Functions by subclassing
    torch.autograd.Function and implementing the forward and backward passes
    which operate on Tensors.
    """

    @staticmethod
    def forward(ctx, input, num_final_select, device):
        """
        In the forward pass we receive a Tensor containing the input and return
        a Tensor containing the output. ctx is a context object that can be used
        to stash information for backward computation. You can cache arbitrary
        objects for use in the backward pass using the ctx.save_for_backward method.
        """
        ctx.save_for_backward(input)
        # rst = torch.where(input>thrd, torch.tensor(1.0, device=device, requires_grad=True),
        #                               torch.tensor(0.0, device=device, requires_grad=True))
        top_values, top_indices = torch.topk(input, num_final_select)
        mask = torch.zeros_like(input).to(device)
        mask[top_indices] = 1
        rst = input * mask
        return rst

    @staticmethod
    def backward(ctx, grad_output):
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        
        """
        Return results number should corresponding with .forward inputs (besides ctx),
        for each input, return a corresponding backward grad
        """
        return grad_input, None, None
